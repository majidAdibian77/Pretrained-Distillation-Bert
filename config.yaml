
data:
  corpus_path: "dataset/raw_data/03_wiki_normalized_tokenized_word_neighbouring.txt"
  processed_path: "dataset/processed_data/wiki_data.txt"
  max_input_tokens: 512
  short_seq_prob: 0.2
  next_sentence_prob: 0.5
  mask_prob: 0.15

model:
  num_layers: 4
  hidden_size: 256
  max_seq_len: 512

tokenizer:
  batch_size: 256
  save_paths: "output/tokenizer"

train:
  device: "cpu"
  val_size: 128
  batch_size: 16
  learning_rate: 0.0001
  weight_decay: 0.0001
  num_train_epochs: 200
  save_step: 1000
  val_steps: 100
  log_step: 100
  ckpt_path: "output/model"
  log_path: "output/log"